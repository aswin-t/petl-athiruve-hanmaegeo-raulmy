@article{houlsby_2019,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin De Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  year={2019},
  publisher={ICML}
}

@article{lester_2021,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Brian Lester and Rami Al-Rfou and Noah Constant},
  year={2021},
  publisher={EMNLP}
}

@article{li_2021,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Xiang Lisa Li and Percy Liang},
  year={2021},
  publisher={ACL}
}

@article{liu_2021,
  title={Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
  year={2021},
  publisher={arXiv}
}

@article{hu_2021,
  title={LoRA: Low-rank Adaptation of Large Language Models},
  author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  year={2021},
  publisher={arXiv}
}

@article{vu_2022,
  title={SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer},
  author={Tu Vu and Brian Lester and Noah Constant and Rami Al-Rfou and Daniel Cer and Google Research},
  year={2022},
  publisher={ACL}
}

@article{raffel_2022,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Colin Raffel and Noam Shazer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  year={2020}
}


